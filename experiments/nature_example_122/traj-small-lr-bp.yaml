main_import_code: |-
    from supervised_learning_trainable import SupervisedLearningTrainable

ray_paradigm: "run"

run_or_experiment: "SupervisedLearningTrainable"

resources_per_trial:
    cpu: 1
    gpu: 0
stop:
    is_num_iterations_reached: 1

config:
    version: 0.1

    device: "torch.device('cpu')"

    seed: 1482555873

    num_iterations:
        grid_search:
            - 1
            - 2
            - 3
            - 4
            - 5
            - 6
            - 7
            - 8
            - 9
            - 10
            - 11
            - 12
            - 13
            - 14
            - 15
            - 16
            - 17
            - 18
            - 19
            - 20
            - 21
            - 22
            - 23
            - 24
            - 25
            - 26
            - 27
            - 28
            - 29
            - 30
            - 31
            - 32
            - 33
            - 34
            - 35
            - 36
            - 37
            - 38
            - 39
            - 40
            - 41
            - 42
            - 43
            - 44
            - 45
            - 46
            - 47
            - 48
            - 49
            - 50
            - 51
            - 52
            - 53
            - 54
            - 55
            - 56
            - 57
            - 58
            - 59
            - 60
            - 61
            - 62
            - 63
            - 64
            - 65
            - 66
            - 67
            - 68
            - 69
            - 70
            - 71
            - 72
            - 73
            - 74
            - 75
            - 76
            - 77
            - 78
            - 79
            - 80
            - 81
            - 82
            - 83
            - 84
            - 85
            - 86
            - 87
            - 88
            - 89
            - 90
            - 91
            - 92
            - 93
            - 94
            - 95
            - 96
            - 97
            - 98
            - 99
            - 100
            - 101
            - 102
            - 103
            - 104
            - 105
            - 106
            - 107
            - 108
            - 109
            - 110
            - 111
            - 112
            - 113
            - 114
            - 115
            - 116
            - 117
            - 118
            - 119
            - 120
            - 121
            - 122
            - 123
            - 124
            - 125
            - 126
            - 127
            - 128
            - 129
            - 130
            - 131
            - 132
            - 133
            - 134
            - 135
            - 136
            - 137
            - 138
            - 139
            - 140
            - 141
            - 142
            - 143
            - 144
            - 145
            - 146
            - 147
            - 148
            - 149
            - 150
            # - 151
            # - 152
            # - 153
            # - 154
            # - 155
            # - 156
            # - 157
            # - 158
            # - 159
            # - 160
            # - 161
            # - 162
            # - 163
            # - 164
            # - 165
            # - 166
            # - 167
            # - 168
            # - 169
            # - 170
            # - 171
            # - 172
            # - 173
            # - 174
            # - 175
            # - 176
            # - 177
            # - 178
            # - 179
            # - 180
            # - 181
            # - 182
            # - 183
            # - 184
            # - 185
            # - 186
            # - 187
            # - 188
            # - 189
            # - 190
            # - 191
            # - 192
            # - 193
            # - 194
            # - 195
            # - 196
            # - 197
            # - 198
            # - 199
            # - 200
            # - 201
            # - 202
            # - 203
            # - 204
            # - 205
            # - 206
            # - 207
            # - 208
            # - 209
            # - 210
            # - 211
            # - 212
            # - 213
            # - 214
            # - 215
            # - 216
            # - 217
            # - 218
            # - 219
            # - 220
            # - 221
            # - 222
            # - 223
            # - 224
            # - 225
            # - 226
            # - 227
            # - 228
            # - 229
            # - 230
            # - 231
            # - 232
            # - 233
            # - 234
            # - 235
            # - 236
            # - 237
            # - 238
            # - 239
            # - 240
            # - 241
            # - 242
            # - 243
            # - 244
            # - 245
            # - 246
            # - 247
            # - 248
            # - 249
            # - 250
            # - 251
            # - 252
            # - 253
            # - 254
            # - 255
            # - 256
            # - 257
            # - 258
            # - 259
            # - 260
            # - 261
            # - 262
            # - 263
            # - 264
            # - 265
            # - 266
            # - 267
            # - 268
            # - 269
            # - 270
            # - 271
            # - 272
            # - 273
            # - 274
            # - 275
            # - 276
            # - 277
            # - 278
            # - 279
            # - 280
            # - 281
            # - 282
            # - 283
            # - 284
            # - 285
            # - 286
            # - 287
            # - 288
            # - 289
            # - 290
            # - 291
            # - 292
            # - 293
            # - 294
            # - 295
            # - 296
            # - 297
            # - 298
            # - 299
            # - 300
            # - 301
            # - 302
            # - 303
            # - 304
            # - 305
            # - 306
            # - 307
            # - 308
            # - 309
            # - 310
            # - 311
            # - 312
            # - 313
            # - 314
            # - 315
            # - 316
            # - 317
            # - 318
            # - 319
            # - 320
            # - 321
            # - 322
            # - 323
            # - 324
            # - 325
            # - 326
            # - 327
            # - 328
            # - 329
            # - 330
            # - 331
            # - 332
            # - 333
            # - 334
            # - 335
            # - 336
            # - 337
            # - 338
            # - 339
            # - 340
            # - 341
            # - 342
            # - 343
            # - 344
            # - 345
            # - 346
            # - 347
            # - 348
            # - 349
            # - 350
            # - 351
            # - 352
            # - 353
            # - 354
            # - 355
            # - 356
            # - 357
            # - 358
            # - 359
            # - 360
            # - 361
            # - 362
            # - 363
            # - 364
            # - 365
            # - 366
            # - 367
            # - 368
            # - 369
            # - 370
            # - 371
            # - 372
            # - 373
            # - 374
            # - 375
            # - 376
            # - 377
            # - 378
            # - 379
            # - 380
            # - 381
            # - 382
            # - 383
            # - 384
            # - 385
            # - 386
            # - 387
            # - 388
            # - 389
            # - 390
            # - 391
            # - 392
            # - 393
            # - 394
            # - 395
            # - 396
            # - 397
            # - 398
            # - 399
            # - 400

    batch_size: 1

    dataset_kwargs:
        data: "[[1.0]]"
        target: "[[1.0,1.0]]"

    # exec-code before/after the setup of the specified Trainable
    before_DatasetLearningTrainable_creating_data_packs_code: |-
        def data_loader_fn(batch_size, dataset_kwargs={}):
            
            data = eval(dataset_kwargs['data'])
            target = eval(dataset_kwargs['target'])

            dataset = TensorDataset(
                torch.Tensor(data),
                torch.Tensor(target),
            )

            return DataLoader(
                dataset,
                batch_size=batch_size,
                pin_memory=True,
                shuffle=True,
                drop_last=True,
            )

    data_packs:
        train:
            data_loader: |-
                data_loader_fn(
                    batch_size=self.config['batch_size'],
                    dataset_kwargs=self.config['dataset_kwargs'],
                )
            do: "['learn']"

    predictive_coding: False

    T: 128

    PCTrainer_kwargs:
        update_x_at: "all"
        optimizer_x_fn: "SGD"
        optimizer_x_kwargs:
            lr: 0.1
        x_lr_discount: 0.8
        x_lr_amplifier: 1.0

        update_p_at: "last"
        optimizer_p_fn: "SGD"
        optimizer_p_kwargs:
            lr:
                grid_search:
                    # - 0.0
                    # - 1.0
                    # - 0.6
                    # - 0.4
                    # - 0.2
                    # - 0.1
                    - 0.05
                    # - 0.01
                    # - 0.005
                    # - 0.001
                    # - 0.0005
                    # - 0.0001

        T: "self.config['T'] if self.config['predictive_coding'] else 1"

        plot_progress_at: "[]"
        # debug
        # plot_progress_at: "np.arange(0,64*600*10,6000).tolist()"

    Layer1_init_kwargs:
        w_1: 0.0
        w_2: 0.0
    Layer2_kwargs:
        w_1: 1.0
        w_2: 2.0

    model_creation_code: |-
        # import
        import predictive_coding as pc
        import torch.optim as optim
        import experiments.nature_example_122.utils as u

        # create model

        self.layer1 = u.Layer1(**self.config['Layer1_init_kwargs'])
        self.layer2 = u.Layer2(**self.config['Layer2_kwargs'])
        self.model = [
            self.layer1,
            pc.PCLayer(),
            self.layer2,
        ]

        # decide pc_layer
        for model_ in self.model:
            if isinstance(model_, pc.PCLayer):
                if not self.config['predictive_coding']:
                    self.model.remove(model_)

        # create sequential
        self.model = nn.Sequential(*self.model).to(self.device)

        # create pc_trainer kwargs
        self.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(
            'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])
        )
        self.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(
            'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])
        )
        self.config['PCTrainer_kwargs']['T']=eval(
            self.config['PCTrainer_kwargs']['T']
        )
        self.config['PCTrainer_kwargs']['plot_progress_at']=eval(
            self.config['PCTrainer_kwargs']['plot_progress_at']
        )

        # create pc_trainer
        self.pc_trainer = pc.PCTrainer(
            self.model,
            **self.config['PCTrainer_kwargs'],
        )

    # predict_code: |-

    #     self.model.eval()
    #     prediction = self.model(data)
    #     self.loss = (
    #         prediction-target
    #     ).pow(2).sum()

    train_on_batch_kwargs:
        is_log_progress: False
        is_return_results_every_t: False
        # debug
        # is_return_results_every_t: True
        is_checking_after_callback_after_t: False

    learn_code: |-
        self.model.train()

        def loss_fn(outputs, target):
            return (outputs - target).pow(2).sum() * 0.5

        self.w_1 = self.layer1.w_1.item()
        self.w_2 = self.layer1.w_2.item()

        # w_before = torch.tensor([self.layer1.w_1.item(), self.layer1.w_2.item()])

        self.pc_trainer.train_on_batch(
            data, loss_fn,
            loss_fn_kwargs={
                'target': target,
            },
            **self.config['train_on_batch_kwargs'],
        )

        w_after = torch.tensor([self.layer1.w_1.item(), self.layer1.w_2.item()])

        # step_size = (w_after - w_before).norm(2)

        # input(step_size)

    # !debug
    log_packs:
        w_1:
            log: "self.w_1"
            at_data_pack: "['train']"
        w_2:
            log: "self.w_2"
            at_data_pack: "['train']"
        # loss:
        #     log: "self.loss.item()"
        #     at_data_pack: "['train']"
