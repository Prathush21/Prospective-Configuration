run_or_experiment: "SupervisedLearningTrainable"

resources_per_trial:
    cpu: 1
    gpu: 0
stop:
    is_stop: 1
max_failures: 1
fail_fast: False
checkpoint_freq: 0
checkpoint_at_end: False

config:

    deterministic: True # setup for deterministic behaviour as possible (trial level)

    seed: # seed
        grid_search:
            - 1482555873
            - 698841058
            - 2283198659
            # - 1855152618
            # - 2444264003
            # - 4191194675
            # - 2563230118
            # - 2508066235
            # - 690101352
            # - 1489128536
            # - 193493729
            # - 3247095100
            # - 2569134589
            # - 3859394752
            # - 3982413761
            # - 2889788203
            # - 3507183834
            # - 1288605031
            # - 1603403502
            # - 4110491292
            # - 3228301166
            # - 1625209643
            # - 3229422879
            # - 4117841395
            # - 383017664
            # - 369370935
            # - 2278516753
            # - 2326123831
            # - 3558864999
            # - 281470168
            # - 3965936537
            # - 1095454788
            # - 4151697083
            # - 3549593167
            # - 3621705125
            # - 2951949230
            # - 2942809220
            # - 1412354999
            # - 3653984540
            # - 38155820
            # - 452984486
            # - 2219886835
            # - 1824037622
            # - 1223472929
            # - 375839252
            # - 2597045926
            # - 187775831
            # - 2291831353
            # - 3551958879
            # - 760971382
            # - 1019978323
            # - 3385238229
            # - 2124033150
            # - 2909826692
            # - 3761144171
            # - 2586511809
            # - 2821469938
            # - 3244598120
            # - 1195937429
            # - 3800305993
            # - 1106674707
            # - 1922347502
            # - 2999545244
            # - 1650175939
            # - 3200709490
            # - 1947803234
            # - 301456582
            # - 1611073380
            # - 3238577641
            # - 1446155378
            # - 1705511488
            # - 2777770570
            # - 3913116301
            # - 1525032703
            # - 3260116528
            # - 3235491768
            # - 2021899074
            # - 550305527
            # - 2227549273
            # - 3227763636
            # - 4034863635
            # - 2984716302
            # - 822586165
            # - 2244632731
            # - 2578193189
            # - 2806006426
            # - 364049366
            # - 2580805533
            # - 1471857781
            # - 636897984
            # - 3061662337
            # - 3640170982
            # - 3927284778
            # - 3117797531
            # - 1117650596
            # - 223429686
            # - 651134664
            # - 955904314
            # - 1703657804
            # - 2162018890

    device: "torch.device('cpu')"

    iteration_end:
        grid_search:
            - 10
            - 100

    num_datapoints:
        grid_search:
            - 1
            - 4
            - 8

    batch_size: 1

    data_packs:
        train:
            data_loader: |-
                DataLoader(
                    TensorDataset(
                        torch.zeros(
                            int(self.config['num_datapoints']), 32
                        ).normal_(0.0,0.1),
                        torch.zeros(
                            int(self.config['num_datapoints']), 32
                        ).normal_(0.0,0.1),
                    ),
                    batch_size=self.config['batch_size'],
                    num_workers=1,
                    pin_memory=True,
                    shuffle=True,
                    drop_last=False,
                )
            do: "['learn']"

    ns: "[32,32,32,32]"

    init_fn: "torch.nn.init.xavier_normal_"

    predictive_coding:
        grid_search:
            - True
            - False

    PCTrainer_kwargs:
        update_x_at: "all"
        optimizer_x_fn: "optim.SGD"
        optimizer_x_kwargs:
            lr: 0.1
        x_lr_discount: 0.5

        update_p_at: "last"
        optimizer_p_fn: "optim.SGD"
        optimizer_p_kwargs:
            lr:
                grid_search:
                    - 0.1
                    - 0.05
                    - 0.01
                    - 0.005
                    - 0.001
                    - 0.0005
                    - 0.0001
                    - 0.00005
                    - 0.00001

        T: "500 if self.config['predictive_coding'] else 1"

        plot_progress_at: "[]"

    measure:
        grid_search:
            - angle_step
            - angle_accum
            - traj_rate

    model_creation_code: |-

        # import
        import predictive_coding as pc
        import torch.optim as optim

        # create model
        self.ns = eval(self.config["ns"])
        assert len(self.ns) == 4
        self.model = [
            nn.Linear(self.ns[0], self.ns[1], bias=False),
            pc.PCLayer(),
            nn.Sigmoid(),
            nn.Linear(self.ns[1], self.ns[2], bias=False),
            pc.PCLayer(),
            nn.Sigmoid(),
            nn.Linear(self.ns[2], self.ns[3], bias=False),
        ]

        # decide pc_layer
        for model_ in self.model:
            if isinstance(model_, pc.PCLayer):
                if not self.config['predictive_coding']:
                    self.model.remove(model_)

        # init
        for model_ in self.model:
            if isinstance(model_, nn.Linear):
                eval(self.config['init_fn'])(model_.weight)

        # create sequential
        self.model = nn.Sequential(*self.model).to(self.device)

        # create pc_trainer
        self.config["PCTrainer_kwargs"]["optimizer_x_fn"]=eval(
            self.config["PCTrainer_kwargs"]["optimizer_x_fn"]
        )
        self.config["PCTrainer_kwargs"]["optimizer_p_fn"]=eval(
            self.config["PCTrainer_kwargs"]["optimizer_p_fn"]
        )
        self.config["PCTrainer_kwargs"]["T"]=eval(
            self.config["PCTrainer_kwargs"]["T"]
        )
        self.config["PCTrainer_kwargs"]["plot_progress_at"]=eval(
            self.config["PCTrainer_kwargs"]["plot_progress_at"]
        )
        self.pc_trainer = pc.PCTrainer(
            self.model,
            **self.config["PCTrainer_kwargs"],
        )

        def get_weights(model):
            weights = []
            for model_ in model:
                if isinstance(model_, nn.Linear):
                    weights.append(
                        model_.weight.data.clone()
                    )
            return weights

        self.get_weights = get_weights

        def get_traj_lengths(v_start, v_end):
            assert len(v_start) == len(v_end)
            traj_lengths = []
            for i in range(len(v_start)):
                traj_lengths.append(
                    (v_end[i].view(-1) - v_start[i].view(-1)).norm().item()
                )
            return traj_lengths

        self.get_traj_lengths = get_traj_lengths

        def get_directions(v_start, v_end):
            assert len(v_start) == len(v_end)
            directions = []
            for i in range(len(v_start)):
                directions.append(
                    (v_end[i] - v_start[i]).clone()
                )
            return directions

        self.get_directions = get_directions

        def get_consistencies(v1, v2):
            assert len(v1) == len(v2)
            consistencies = []
            for i in range(len(v1)):
                consistencies.append(
                    torch.nn.functional.cosine_similarity(v1[i].view(-1), v2[i].view(-1), dim=0).item()
                )
            return consistencies

        self.get_consistencies = get_consistencies

        self.weights_history = [self.get_weights(self.model)]

        self.measure_result = None

    train_on_batch_kwargs:
        is_log_progress: False

    learn_code: |-

        self.model.train()

        def loss_fn(outputs, target):
            return (outputs - target).pow(2).sum() * 0.5

        results = self.pc_trainer.train_on_batch(
            data, loss_fn,
            loss_fn_kwargs={
                "target": target,
            },
            **self.config["train_on_batch_kwargs"],
        )

        self.weights_history.append(
            self.get_weights(self.model)
        )

    after_iteration_data_packs_code: |-

        if self._iteration == (self.config['iteration_end'] - 1):

            result_dict['is_stop'] = 1

            if self.config['measure'] in ['traj_rate']:

                final_traj_length = np.array(
                    self.get_traj_lengths(self.weights_history[0], self.weights_history[-1])
                )

                step_traj_length = []
                for i in range(0, len(self.weights_history)-1):
                    step_traj_length.append(
                        self.get_traj_lengths(self.weights_history[i], self.weights_history[i+1])
                    )
                step_traj_length = np.array(step_traj_length)
                step_traj_length = np.sum(
                    step_traj_length,
                    axis=0, keepdims=False,
                )

                self.measure_result = np.mean(step_traj_length/final_traj_length).item()

            elif self.config['measure'] in ['angle_step','angle_accum']:

                final_directions = self.get_directions(self.weights_history[0], self.weights_history[-1])

                consistencies = []
                for i in range(0, len(self.weights_history)-1):
                    start_i = {
                        'angle_step': i,
                        'angle_accum': 0,
                    }[self.config['measure']]
                    consistencies.append(
                        self.get_consistencies(
                            self.get_directions(
                                self.weights_history[start_i], self.weights_history[i+1]
                            ),
                            final_directions
                        )
                    )

                self.measure_result = np.mean(np.array(consistencies)).item()

        else:

            result_dict['is_stop'] = 0

        result_dict['measure_result'] = self.measure_result
