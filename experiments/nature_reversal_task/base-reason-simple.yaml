run_or_experiment: "SupervisedLearningTrainable"

resources_per_trial:
    cpu: 1
    gpu: 0
stop:
    training_iteration: 64

config:

    deterministic: True # setup for deterministic behaviour as possible (trial level)

    seed: 1482555873

    device: "torch.device('cpu')"

    data_packs:
        train:
            data_loader: |-
                DataLoader(
                    TensorDataset(
                        torch.Tensor(
                            [[1.0]]
                        ),
                        torch.Tensor(
                            [[-1.0,1.0]]
                        ),
                    ),
                    batch_size=1,
                )
            do: "['learn']"

    ns: "[1,1,2]"

    predictive_coding:
        grid_search:
            - True
            - False

    PCTrainer_kwargs:
        update_x_at: "all"
        optimizer_x_fn: "optim.SGD"
        optimizer_x_kwargs:
            lr: 0.1
        x_lr_discount: 0.9

        update_p_at: "last"
        optimizer_p_fn: "optim.SGD"
        optimizer_p_kwargs:
            lr:
                grid_search:
                    - 0.1
                    - 0.05
                    - 0.01

        T: "256 if self.config['predictive_coding'] else 1"

        plot_progress_at: "[]"
        # debug
        # plot_progress_at: "list(range(0,16,1))"


    model_creation_code: |-

        # import
        import predictive_coding as pc
        import torch.optim as optim

        # create model
        self.ns = eval(self.config["ns"])
        self.model = [
            nn.Linear(1, 1, bias=False),
            pc.PCLayer(),
            nn.Linear(1, 2, bias=False),
        ]

        # decide pc_layer
        for model_ in self.model:
            if isinstance(model_, pc.PCLayer):
                if not self.config['predictive_coding']:
                    self.model.remove(model_)

        # init
        self.linears = []
        for model_ in self.model:
            if isinstance(model_, nn.Linear):
                self.linears.append(model_)

        self.linears[0].weight.data[0,0].fill_(1.0)
        self.linears[1].weight.data[0,0].fill_(1.0)
        self.linears[1].weight.data[1,0].fill_(-1.0)

        # create sequential
        self.model = nn.Sequential(*self.model).to(self.device)

        # create pc_trainer
        self.config["PCTrainer_kwargs"]["optimizer_x_fn"]=eval(
            self.config["PCTrainer_kwargs"]["optimizer_x_fn"]
        )
        self.config["PCTrainer_kwargs"]["optimizer_p_fn"]=eval(
            self.config["PCTrainer_kwargs"]["optimizer_p_fn"]
        )
        self.config["PCTrainer_kwargs"]["T"]=eval(
            self.config["PCTrainer_kwargs"]["T"]
        )
        self.config["PCTrainer_kwargs"]["plot_progress_at"]=eval(
            self.config["PCTrainer_kwargs"]["plot_progress_at"]
        )
        self.pc_trainer = pc.PCTrainer(
            self.model,
            **self.config["PCTrainer_kwargs"],
        )

    train_on_batch_kwargs:
        is_log_progress: False
        is_return_results_every_t: False
        is_checking_after_callback_after_t: False

    learn_code: |-

        action = 0

        self.model.train()

        def loss_fn(outputs, action, target):
            return (outputs[:,action:action+1] - target[:,action:action+1]).pow(2).sum() * 0.5

        self.pc_trainer.train_on_batch(
            data, loss_fn,
            loss_fn_kwargs={
                "action": action,
                "target": target,
            },
            **self.config["train_on_batch_kwargs"],
        )


    which_weight:
        grid_search:
            # - 'w0-00'
            - 'w1-00'
            # - 'w1-10'

    after_iteration_data_packs_code: |-

        result_dict['weight'] = {
            'w0-00': self.linears[0].weight[0,0].data.item(),
            'w1-00': self.linears[1].weight[0,0].data.item(),
            'w1-10': self.linears[1].weight[1,0].data.item(),
        }[self.config['which_weight']]
