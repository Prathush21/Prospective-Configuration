run_or_experiment: "SupervisedLearningTrainable"
resources_per_trial:
  cpu: 1
  gpu: 0.25

stop:
  is_num_iterations_reached: 1

checkpoint_freq: 0
checkpoint_at_end: False

config:
  version: 1.3
  device: "torch.device('cuda')"

  seed:
    grid_search:
      - 698841058

  num_iterations: 20
  dataset: Imagenette
  partial_num: 100
  batch_size: 200

  before_DatasetLearningTrainable_setup_code: |-
    from experiments.test_2.CustomDataset import CustomDataset

    def data_loader_fn(dataset, train, batch_size, partial_num=-1):
        transform = transforms.Compose([
            transforms.Resize((32, 32)),
            transforms.ToTensor(),
        ])

        target_transform = transforms.Lambda(
            lambda idx: torch.eye(10)[idx]
        )

        return DataLoader(
            dataset_utils_imagenette.partial_dateset(
                CustomDataset(
                    root="/content/drive/MyDrive/advanced_ai_project/RESULTS_DIR/imagenette2",
                    train=train,
                    transform=transform,
                    target_transform=target_transform
                ),
                partial_num=partial_num,
            ),
            batch_size=batch_size,
            num_workers=1,
            pin_memory=True,
            shuffle=True,
            drop_last=True,
        )

  data_packs:
    train:
      data_loader: |-
        data_loader_fn(
            dataset=self.config['dataset'],
            train=True,
            batch_size=self.config['batch_size'],
            partial_num=self.config['partial_num'],
        )
      do: "['learn']"
    test:
      data_loader: |-
        data_loader_fn(
            dataset=self.config['dataset'],
            train=False,
            batch_size=self.config['batch_size'],
        )
      do: "['predict']"

  predictive_coding:
    grid_search:
      - True
      - False

  PCTrainer_kwargs:
    update_x_at: "all"
    optimizer_x_fn: "SGD"
    optimizer_x_kwargs:
      lr: 0.5
    x_lr_discount: 0.5
    x_lr_amplifier: 1.0

    update_p_at: "all"
    optimizer_p_fn: "Adam"
    optimizer_p_kwargs:
      lr:
        grid_search:
          - 0.00025
          - 0.0001
          - 0.000075
          - 0.00005
          - 0.000025
          - 0.00001
      weight_decay:
        grid_search:
          - 0.01

    T: 16
    plot_progress_at: "np.arange(0,64*600*10,6000).tolist()"

  model:
    acf: "torch.nn.ReLU"
    model_type_order: 
      grid_search:
        - "['Weights', 'PCLayer', 'Acf', 'Pool']"
        - "['Weights', 'Acf', 'PCLayer', 'Pool']"
        - "['Weights', 'Acf', 'Pool', 'PCLayer']"
    cnn_layers:
      cnn_0:
        fn: "torch.nn.Conv2d"
        kwargs:
          in_channels: 3
          out_channels: 32
          kernel_size: 3
          stride: 1
          padding: 0
          bias: True

      cnn_1:
        fn: "torch.nn.Conv2d"
        kwargs:
          in_channels: 32
          out_channels: 64
          kernel_size: 3
          stride: 1
          padding: 0
          bias: True

      cnn_2:
        fn: "torch.nn.Conv2d"
        kwargs:
          in_channels: 64
          out_channels: 64
          kernel_size: 3
          stride: 1
          padding: 0
          bias: True

    linear_layers:
      linear_0:
        fn: "torch.nn.Linear"
        kwargs:
          in_features: 1024 # 64 * 28 * 28 (calculate based on final conv layer output size)
          out_features: 64
          bias: True
      last:
        fn: "torch.nn.Linear"
        kwargs:
          in_features: 64
          out_features: 10
          bias: True

  model_creation_code: |-
    import predictive_coding as pc
    import torch.optim as optim
    import experiments.test_2.utils as u

    self.model = u.create_model(
        self.config['predictive_coding'],
        **self.config['model'],
    ).to(self.device)

    self.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(
        'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])
    )
    self.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(
        'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])
    )
    self.config['PCTrainer_kwargs']['plot_progress_at']=eval(
        self.config['PCTrainer_kwargs']['plot_progress_at']
    )

    self.pc_trainer = pc.PCTrainer(
        self.model,
        **self.config['PCTrainer_kwargs'],
    )

  predict_code: |-
    self.model.eval()
    prediction = self.model(data)
    self.classification_error = utils.get_classification_error(
        prediction, target
    )

  train_on_batch_kwargs:
    is_log_progress: False
    is_return_results_every_t: False
    is_checking_after_callback_after_t: False

  learn_code: |-
    self.model.train()

    def loss_fn(outputs, target):
        return (outputs - target).pow(2).sum() * 0.5

    self.pc_trainer.train_on_batch(
        data, loss_fn,
        loss_fn_kwargs={
            'target': target,
        },
        **self.config['train_on_batch_kwargs'],
    )

  log_packs:
    classification_error:
      log: "self.classification_error.item()"
      at_data_pack: "['test']"
