run_or_experiment: "SupervisedLearningTrainable"

resources_per_trial:
    cpu: 1
    gpu: "fit_cpu"
    # debug
    # gpu: 0
stop:
    is_num_iterations_reached: 1
checkpoint_freq: 0
checkpoint_at_end: False

config:
    version: 0.1

    device: "torch.device('cuda')"
    # debug
    # device: "torch.device('cpu')"

    seed:
        grid_search:
            - 1482555873
            - 698841058
            - 2283198659

    num_iterations: 64
    # debug
    # num_iterations: 3

    dataset: FashionMNIST

    # partial_num: 6000
    # debug
    partial_num: 60

    batch_size: 60

    # exec-code before/after the setup of the specified Trainable
    before_DatasetLearningTrainable_setup_code: |-

        def data_loader_fn(dataset, train, batch_size, partial_num=-1):

            transform = []
            if dataset in ['CIFAR10']:
                transform.append(transforms.Grayscale())
            transform.append(transforms.ToTensor())
            transform.append(utils.transforms_flatten)

            target_transform = []
            target_transform.append(
                transforms.Lambda(
                    lambda idx: utils.np_idx2onehot(idx, 10)
                )
            )

            return DataLoader(
                dataset_utils.partial_dateset(
                    eval(
                        'datasets.{}'.format(dataset)
                    )(
                        os.environ.get('DATA_DIR'),
                        train=train,
                        download=False,
                        transform=transforms.Compose(transform),
                        target_transform=transforms.Compose(target_transform)
                    ),
                    partial_num=partial_num,
                ),
                batch_size=batch_size,
                num_workers=1,
                pin_memory=True,
                shuffle=True,
            )

    data_packs:
        train:
            data_loader: |-
                data_loader_fn(
                    dataset=self.config['dataset'],
                    train=True,
                    batch_size=self.config['batch_size'],
                    partial_num=self.config['partial_num'],
                )
            do: "['learn']"
        # !debug
        test:
            data_loader: |-
                data_loader_fn(
                    dataset=self.config['dataset'],
                    train=False,
                    batch_size=self.config['batch_size'],
                )
            do: "['predict']"

    predictive_coding:
        grid_search:
            - True
            - False

    T: 128

    PCTrainer_kwargs:
        update_x_at: "all"
        optimizer_x_fn: "SGD"
        optimizer_x_kwargs:
            lr: 0.1
        x_lr_discount: 0.5
        x_lr_amplifier: 1.0

        update_p_at: "last"
        optimizer_p_fn: "SGD"
        optimizer_p_kwargs:
            lr:
                grid_search:
                    - 5.0
                    - 1.0
                    - 0.5
                    - 0.1
                    - 0.05
                    - 0.01
                    - 0.005
                    - 0.001
                    # - 0.0005
                    # - 0.0001
                    # - 0.00005
                    # - 0.00001
                    # - 0.000005

        T: "self.config['T'] if self.config['predictive_coding'] else 1"

        plot_progress_at: "[]"
        # debug
        # plot_progress_at: "np.arange(0,64*600*10,6000).tolist()"

    acf:
        grid_search:
            # - nn.Sigmoid() # deos not work with deep networks
            # - nn.ReLU() # pattern does not hold
            # - pc.NoneModule() # pattern only holds for num_layers<6
            - nn.LeakyReLU()
            # - nn.Tanh() # wierd with deep nets

    structure: "['Linear', 'PCLayer', 'Acf']"

    norm_layer: "None()"

    num_layers:
        grid_search:
            # - 1
            # - 2
            # # - 3
            # - 4
            # # - 5
            # - 6
            # # - 7
            # - 8
            # # - 9
            # - 10
            # # - 11
            # - 12
            # # - 13
            # - 14
            - 15
            # - 16
            # - 18
            # - 20

    hidden_size: 64

    init_fn:
        grid_search:
            # - "torch.nn.init.xavier_uniform_" # no difference from the other
            - "torch.nn.init.xavier_normal_"

    bias: False

    gain_lg: 0.0

    model_creation_code: |-

        # import
        import predictive_coding as pc
        import torch.optim as optim

        import experiments.nature_search_depth.utils as u

        # create model

        input_size = {
            'FashionMNIST': 784,
            'CIFAR10': 1024,
        }[self.config['dataset']]

        hidden_size = self.config['hidden_size']

        self.model = []
        for l in range(self.config['num_layers']-1):
            for model_type in eval(self.config['structure']):
                if model_type=='Linear':
                    self.model.append(
                        nn.Linear(
                            input_size if l==0 else hidden_size,
                            hidden_size,
                            bias=self.config['bias'],
                        )
                    )
                elif model_type=='PCLayer':
                    self.model.append(
                        pc.PCLayer(),
                    )
                elif model_type=='Acf':
                    self.model.append(
                        eval(self.config['acf'])
                    )
                elif model_type=='Norm':
                    h = hidden_size
                    self.model.append(
                        eval('nn.{}'.format(self.config['norm_layer'])),
                    )
                else:
                    raise NotImplementedError
        self.model.append(
            nn.Linear(
                hidden_size if (self.config['num_layers'] > 1) else input_size, 10,
                bias=self.config['bias'],
            )
        )

        # decide pc_layer
        for model_ in self.model:
            if isinstance(model_, pc.PCLayer):
                if not self.config['predictive_coding']:
                    self.model.remove(model_)

        # init
        for model_ in self.model:
            if isinstance(model_, nn.Linear):
                eval(self.config['init_fn'])(
                    model_.weight,
                    gain=u.acf2gain[self.config['acf']]*(2**self.config['gain_lg']),
                )

        # create sequential
        self.model = nn.Sequential(*self.model).to(self.device)

        # create pc_trainer
        self.config['PCTrainer_kwargs']['optimizer_x_fn']=eval(
            'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_x_fn'])
        )
        self.config['PCTrainer_kwargs']['optimizer_p_fn']=eval(
            'optim.{}'.format(self.config['PCTrainer_kwargs']['optimizer_p_fn'])
        )
        self.config['PCTrainer_kwargs']['T']=eval(
            self.config['PCTrainer_kwargs']['T']
        )
        self.config['PCTrainer_kwargs']['plot_progress_at']=eval(
            self.config['PCTrainer_kwargs']['plot_progress_at']
        )
        self.pc_trainer = pc.PCTrainer(
            self.model,
            **self.config['PCTrainer_kwargs'],
        )

    predict_code: |-

        self.model.eval()
        prediction = self.model(data)
        self.classification_error = utils.get_classification_error(
            prediction, target
        )

    train_on_batch_kwargs:
        is_log_progress: False
        is_return_results_every_t: False
        # debug
        # is_return_results_every_t: True
        is_checking_after_callback_after_t: False

    learn_code: |-

        self.model.train()

        def loss_fn(outputs, target):
            return (outputs - target).pow(2).sum() * 0.5

        self.pc_trainer.train_on_batch(
            data, loss_fn,
            loss_fn_kwargs={
                'target': target,
            },
            **self.config['train_on_batch_kwargs'],
        )

    # !debug
    log_packs:
        classification_error:
            log: "self.classification_error.item()"
            at_data_pack: "['test']"
